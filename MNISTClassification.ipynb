{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH2504 Semester 2, 2022 -  Project 3\n",
    "\n",
    "## Task 2 - Basic ML on MNIST and FashionMNIST\n",
    "\n",
    "Student name : Brandon Lowe <br />\n",
    "Student ID : 43162950 <br />\n",
    "<p><a href=\"https://github.com/Jaanlo/Brandon-Lowe-2504-2022-PROJECT3\">GitHub Repo</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/University/MATH2504/project-3/Brandon-Lowe-2504-2022-PROJECT3`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "include(\"src/mnist-classification/dependencies.jl\"); # dependencies for task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 One vs. all (rest) Linear and Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple linear regression model is given by\n",
    "\n",
    "$$ \\hat{y} = \\alpha + \\beta x $$\n",
    "\n",
    "Extending this to the MNIST dataset, the formula can be considered as,\n",
    "\n",
    "$$ \\hat{y}^{(i)} = \\beta_0 + \\sum_{j=1}^{784}\\beta_j x_j^{(i)} $$\n",
    "\n",
    "We'd like to find a \"good\" estimate $ \\beta \\in \\mathbb{R}^{785} $ by minimizing the quadratic loss. We form a $ n \\times p $ matrix $ A $ called a **design matrix** such that \n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} &  \\ldots & x_p^{(1)} \\\\ \n",
    "1 & x_1^{(2)} & x_2^{(2)} &  \\ldots & x_p^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n",
    "1 & x_1^{(n)} & x_2^{(n)} &  \\ldots & x_p^{(n)} \\end{bmatrix} $$\n",
    "\n",
    "Using this notation, we can now express the linear model via\n",
    "\n",
    "$$ \\hat{y} = A\\hat{\\theta} $$\n",
    "\n",
    "First, lets add some variables for ease of use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels\n",
    "test_imgs = MNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for linear regression our loss function can be represented as,\n",
    "\n",
    "$$ L(\\theta) = ||y - A\\theta||^2 = (y - A\\theta)^\\top (y - A\\theta) $$\n",
    "\n",
    "with the gradient being,\n",
    "\n",
    "$$ \\nabla L(\\theta) = -2A^\\top y + 2A^\\top A\\theta $$\n",
    "\n",
    "which has the unique solution when the matrix $ A^\\top A $ is invertible, which we know here that A is\n",
    "\n",
    "$$ \\hat{\\theta} = A^\\dag y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model for MNIST test images: 0.8603\n"
     ]
    }
   ],
   "source": [
    "MNIST_linear_acc = train_linear()\n",
    "println(\"Accuracy of model for MNIST test images: \", MNIST_linear_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking now at the Fashion MNIST dataset, we use a similar approach to above for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"T-Shirt\"\n",
       " \"Trouser\"\n",
       " \"Pullover\"\n",
       " \"Dress\"\n",
       " \"Coat\"\n",
       " \"Sandal\"\n",
       " \"Shirt\"\n",
       " \"Sneaker\"\n",
       " \"Bag\"\n",
       " \"Ankle boot\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = FashionMNIST.classnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know we have 10 classes in this dataset, similar to the MNIST dataset. Moving ahead with the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels\n",
    "test_imgs = fMNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model for MNIST test images: 0.8113\n"
     ]
    }
   ],
   "source": [
    "fMNIST_linear_acc = train_linear()\n",
    "println(\"Accuracy of model for MNIST test images: \", fMNIST_linear_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the logistic regression model. From <a href=\"https://deeplearningmath.org/\">The Mathematical Engineering of Deep Learning</a> Chapter 3, we are given that the logistic regression model can be represented as, \n",
    "\n",
    "$$ \\hat{y} = \\sigma (b + w^\\top x) \\quad \\textrm{where,} \\\\\n",
    "\\sigma(z) =\\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim minimize the cross-entropy loss which is given by,\n",
    "\n",
    "$$ \n",
    "L(w) = -\\sum_{i=1}^{N}(y^{(i)}\\log{\\hat{y}^{(i)}} + (1- y^{(i)})\\log{(1-\\hat{y}^{(i)})}) \\\\\n",
    "\\nabla L(w) = X^\\top (\\hat{Y} - Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out a function using gradient descent specific for training logistic regression models. I chose to use the loss function from Flux.jl for simplicity. Explicit gradient term has been programmed as per project details. These can be found in `/mnist-classification/logistic-regression.jl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the data for MNIST dataset to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "train_labels = MNIST_train_labels;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the train model function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.66 sec) Loss = 0.2085357315476932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.24 sec) Loss = 0.15744168722529453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.29 sec) Loss = 0.1366063471037671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.32 sec) Loss = 0.12440842067604052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.25 sec) Loss = 0.11610293517223355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.25 sec) Loss = 0.10997874547508622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.27 sec) Loss = 0.10523106867424734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.26 sec) Loss = 0.10141561082469965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.26 sec) Loss = 0.09826605185033173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.26 sec) Loss = 0.09561256862291026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.26 sec) Loss = 0.09334067468328755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.26 sec) Loss = 0.09136972199085368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.27 sec) Loss = 0.08964089817142187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.29 sec) Loss = 0.08811012253331892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.26 sec) Loss = 0.08674361141088945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.25 sec) Loss = 0.08551497880329456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.27 sec) Loss = 0.08440327734954925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.27 sec) Loss = 0.08339164331231777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.29 sec) Loss = 0.08246633905373343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.24 sec) Loss = 0.0816160606868035\n"
     ]
    }
   ],
   "source": [
    "w = train_logistic(η=0.001);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = [ones(n_test) X_test]\n",
    "MNIST_logistic_acc = mean([logistic_classify(T'[:, k], w) for k in 1:n_test] .== MNIST_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the data for the Fashion MNIST dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels = fMNIST_train_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.27 sec) Loss = 0.3481510752147025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.3 sec) Loss = 0.274571224165549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.31 sec) Loss = 0.2415321441444385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.24 sec) Loss = 0.22163432203031005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.25 sec) Loss = 0.20764577476459456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.27 sec) Loss = 0.19688177367035947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.28 sec) Loss = 0.18821392803484874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.25 sec) Loss = 0.18104585754514801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.25 sec) Loss = 0.17500293269528266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.4 sec) Loss = 0.16983014505449692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.32 sec) Loss = 0.16534788601275996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.32 sec) Loss = 0.1614246712620103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.32 sec) Loss = 0.15796040843200762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.5 sec) Loss = 0.1548770105100023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.24 sec) Loss = 0.15211280471507488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.25 sec) Loss = 0.14961864643653713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.25 sec) Loss = 0.14735498443325878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.24 sec) Loss = 0.1452896447047561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.25 sec) Loss = 0.1433961821916812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.26 sec) Loss = 0.14165265622529172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 21 (0.24 sec) Loss = 0.1400407112982488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 22 (0.25 sec) Loss = 0.13854487726161907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 23 (0.25 sec) Loss = 0.13715202947073515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 24 (0.24 sec) Loss = 0.1358509676717609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 25 (0.25 sec) Loss = 0.13463208458502762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 26 (0.24 sec) Loss = 0.13348710318962217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 27 (0.25 sec) Loss = 0.132408867201362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 28 (0.25 sec) Loss = 0.13139117309365117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 29 (0.24 sec) Loss = 0.13042863480554384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 30 (0.24 sec) Loss = 0.1295165742817189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 31 (0.24 sec) Loss = 0.12865093234449068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 32 (0.25 sec) Loss = 0.1278281951846645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 33 (0.24 sec) Loss = 0.12704533206246224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 34 (0.24 sec) Loss = 0.12629973968526295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 35 (0.25 sec) Loss = 0.1255891880148511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 36 (0.24 sec) Loss = 0.12491176007430617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 37 (0.24 sec) Loss = 0.12426577210490879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 38 (0.25 sec) Loss = 0.12364964575226937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 39 (0.24 sec) Loss = 0.12306168395040988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 40 (0.25 sec) Loss = 0.12249972425536937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 41 (0.25 sec) Loss = 0.12196082048281731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 42 (0.24 sec) Loss = 0.1214413796243753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 43 (0.25 sec) Loss = 0.12093794603006781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 44 (0.24 sec) Loss = 0.12044779211850862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 45 (0.24 sec) Loss = 0.11996793042017302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 46 (0.25 sec) Loss = 0.11949215647072985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 47 (0.24 sec) Loss = 0.11901195397646666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 48 (0.24 sec) Loss = 0.11852965496825632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 49 (0.25 sec) Loss = 0.11805456033426294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 50 (0.25 sec) Loss = 0.11759065813718819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 51 (0.24 sec) Loss = 0.11713910050950714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 52 (0.25 sec) Loss = 0.11670052778950277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 53 (0.25 sec) Loss = 0.11627587227130934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 54 (0.24 sec) Loss = 0.11586589608895748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 55 (0.27 sec) Loss = 0.11546935140372724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 56 (0.24 sec) Loss = 0.11508172960858487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 57 (0.24 sec) Loss = 0.11469880087156166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 58 (0.25 sec) Loss = 0.11432016174016185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 59 (0.25 sec) Loss = 0.11394735507620656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 60 (0.24 sec) Loss = 0.11358167499539193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 61 (0.25 sec) Loss = 0.11322382650509244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 62 (0.24 sec) Loss = 0.11287412815300503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 63 (0.25 sec) Loss = 0.11253267365941201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 64 (0.24 sec) Loss = 0.11219941972594405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 65 (0.25 sec) Loss = 0.11187423434538218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 66 (0.24 sec) Loss = 0.11155692649666496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 67 (0.24 sec) Loss = 0.11124726725301483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 68 (0.25 sec) Loss = 0.11094500651259753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 69 (0.25 sec) Loss = 0.11064988647661492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 70 (0.24 sec) Loss = 0.11036165173310175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 71 (0.25 sec) Loss = 0.11008005577463574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 72 (0.25 sec) Loss = 0.10980486429875284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 73 (0.25 sec) Loss = 0.10953585610786501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 74 (0.25 sec) Loss = 0.10927282256239598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 75 (0.25 sec) Loss = 0.10901556638470626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 76 (0.25 sec) Loss = 0.10876390033381553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 77 (0.24 sec) Loss = 0.1085176460147921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 78 (0.24 sec) Loss = 0.10827663291245584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 79 (0.24 sec) Loss = 0.1080406976461151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 80 (0.25 sec) Loss = 0.10780968340505706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 81 (0.25 sec) Loss = 0.1075834395180066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 82 (0.24 sec) Loss = 0.10736182111609203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 83 (0.25 sec) Loss = 0.10714468885868908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 84 (0.25 sec) Loss = 0.10693190870055094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 85 (0.25 sec) Loss = 0.10672335168574898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 86 (0.25 sec) Loss = 0.1065188937590126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 87 (0.24 sec) Loss = 0.10631841558854013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 88 (0.24 sec) Loss = 0.10612180239664284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 89 (0.24 sec) Loss = 0.10592894379607257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 90 (0.24 sec) Loss = 0.10573973363083086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 91 (0.24 sec) Loss = 0.10555406982083058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 92 (0.24 sec) Loss = 0.10537185421012174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 93 (0.25 sec) Loss = 0.10519299241859319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 94 (0.25 sec) Loss = 0.10501739369712712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 95 (0.24 sec) Loss = 0.10484497078620779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 96 (0.24 sec) Loss = 0.10467563977796902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 97 (0.24 sec) Loss = 0.10450931998161149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 98 (0.25 sec) Loss = 0.10434593379207079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 99 (0.24 sec) Loss = 0.1041854065617507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100 (0.24 sec) Loss = 0.10402766647508396\n"
     ]
    }
   ],
   "source": [
    "w = train_logistic(η=0.00037, n_epochs=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the accuracy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8139"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = [ones(n_test) X_test]\n",
    "fMNIST_logistic_acc = mean([logistic_classify(T'[:, k], w) for k in 1:n_test] .== fMNIST_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So overall, we have the following accuracy results for linear regression and logistic regression for MNIST and Fashion MNIST datasets, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST linear regression model: 0.8603\n",
      "Accuracy for Fashion MNIST linear regression model: 0.8113\n",
      "Accuracy for MNIST logistic regression model: 0.9035\n",
      "Accuracy for Fashion  MNIST logistic regression model: 0.8139\n"
     ]
    }
   ],
   "source": [
    "println(\"Accuracy for MNIST linear regression model: $MNIST_linear_acc\")\n",
    "println(\"Accuracy for Fashion MNIST linear regression model: $fMNIST_linear_acc\")\n",
    "println(\"Accuracy for MNIST logistic regression model: $MNIST_logistic_acc\")\n",
    "println(\"Accuracy for Fashion  MNIST logistic regression model: $fMNIST_logistic_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 One vs. One Linear and Logisitic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 is unfinished in current state. Currently I was experimenting with only one of the 45 different models. It works (at least I think so) for that one model, but I have yet to figure out a clean (and not RAM hungry) method of building all 45 models. Every time I try my IDE crashes -_-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28×28×6742 Array{Float64, 3}:\n",
       "[:, :, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.25098   0.0941176  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.984314  0.756863   0.0  0.0  0.0\n",
       " ⋮                        ⋮         ⋱                       ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.992157     0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.247059     0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 3] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 6740] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.65098  0.65098  0.0  0.0  0.0\n",
       " ⋮                        ⋮         ⋱                    ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.996078     0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.462745  …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       "\n",
       "[:, :, 6741] =\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0        …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0        …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0509804  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.992157   0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.658824   0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0196078  0.0  0.0  0.0  0.0\n",
       " ⋮                             ⋮          ⋱                  ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0       0.0196078     0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.643137  0.882353   …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.996078  0.992157      0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.556863  0.529412      0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0        …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 6742] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0156863  0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.788235   0.666667  0.0  0.0  0.0\n",
       " ⋮                        ⋮         ⋱                       ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.215686  …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.901961     0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.32549      0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialising img training sets\n",
    "zero_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 0];\n",
    "one_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 1];\n",
    "# two_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 2]\n",
    "# three_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 3]\n",
    "# four_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 4]\n",
    "# five_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 5]\n",
    "# six_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 6]\n",
    "# seven_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 7]\n",
    "# eight_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 8]\n",
    "# nine_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 9];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialising img training set counts\n",
    "n_zero_train = last(size(zero_imgs))\n",
    "n_one_train = last(size(one_imgs));\n",
    "# n_two_train = last(size(two_imgs))\n",
    "# n_three_train = last(size(three_imgs))\n",
    "# n_four_train = last(size(four_imgs))\n",
    "# n_five_train = last(size(five_imgs))\n",
    "# n_six_train = last(size(six_imgs))\n",
    "# n_seven_train = last(size(seven_imgs))\n",
    "# n_eight_train = last(size(eight_imgs))\n",
    "# n_nine_train = last(size(nine_imgs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6742×784 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                 ⋮              \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialising img as vectors\n",
    "zero_imgs_as_vectors = vcat([vec(zero_imgs[:,:,k])' for k in 1:n_zero_train]...);\n",
    "one_imgs_as_vectors = vcat([vec(one_imgs[:,:,k])' for k in 1:n_one_train]...);\n",
    "# two_imgs_as_vectors = vcat([vec(two_imgs[:,:,k])' for k in 1:n_two_train]...)\n",
    "# three_imgs_as_vectors = vcat([vec(three_imgs[:,:,k])' for k in 1:n_three_train]...)\n",
    "# four_imgs_as_vectors = vcat([vec(four_imgs[:,:,k])' for k in 1:n_four_train]...)\n",
    "# five_imgs_as_vectors = vcat([vec(five_imgs[:,:,k])' for k in 1:n_five_train]...)\n",
    "# six_imgs_as_vectors = vcat([vec(six_imgs[:,:,k])' for k in 1:n_six_train]...)\n",
    "# seven_imgs_as_vectors = vcat([vec(seven_imgs[:,:,k])' for k in 1:n_seven_train]...)\n",
    "# eight_imgs_as_vectors = vcat([vec(eight_imgs[:,:,k])' for k in 1:n_eight_train]...)\n",
    "# nine_imgs_as_vectors = vcat([vec(nine_imgs[:,:,k])' for k in 1:n_nine_train]...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the training data classes to classify by different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12665×784 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                 ⋮              \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vcat of One vs One vectors -- zero && others\n",
    "train_data_zero_one_class = vcat(zero_imgs_as_vectors, one_imgs_as_vectors);\n",
    "# train_data_zero_two_class = vcat(zero_imgs_as_vectors, two_imgs_as_vectors)\n",
    "# train_data_zero_three_class = vcat(zero_imgs_as_vectors, three_imgs_as_vectors)\n",
    "# train_data_zero_four_class = vcat(zero_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_zero_five_class = vcat(zero_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_zero_six_class = vcat(zero_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_zero_seven_class = vcat(zero_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_zero_eight_class = vcat(zero_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_zero_nine_class = vcat(zero_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- one && others\n",
    "# train_data_one_two_class = vcat(one_imgs_as_vectors, two_imgs_as_vectors)\n",
    "# train_data_one_three_class = vcat(one_imgs_as_vectors, three_imgs_as_vectors)\n",
    "# train_data_one_four_class = vcat(one_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_one_five_class = vcat(one_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_one_six_class = vcat(one_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_one_seven_class = vcat(one_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_one_eight_class = vcat(one_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_one_nine_class = vcat(one_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- two && others\n",
    "# train_data_two_three_class = vcat(two_imgs_as_vectors, three_imgs_as_vectors)\n",
    "# train_data_two_four_class = vcat(two_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_two_five_class = vcat(two_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_two_six_class = vcat(two_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_two_seven_class = vcat(two_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_two_eight_class = vcat(two_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_two_nine_class = vcat(two_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- three && others\n",
    "# train_data_three_four_class = vcat(three_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_three_five_class = vcat(three_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_three_six_class = vcat(three_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_three_seven_class = vcat(three_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_three_eight_class = vcat(three_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_three_nine_class = vcat(three_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- four && others\n",
    "# train_data_four_five_class = vcat(four_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_four_six_class = vcat(four_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_four_seven_class = vcat(four_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_four_eight_class = vcat(four_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_four_nine_class = vcat(four_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- five && others\n",
    "# train_data_five_six_class = vcat(five_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_five_seven_class = vcat(five_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_five_eight_class = vcat(five_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_five_nine_class = vcat(five_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- six && others\n",
    "# train_data_six_seven_class = vcat(six_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_six_eight_class = vcat(six_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_six_nine_class = vcat(six_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- seven && others\n",
    "# train_data_seven_eight_class = vcat(seven_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_seven_nine_class = vcat(seven_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- eight && others\n",
    "# train_data_eight_nine_class = vcat(eight_imgs_as_vectors, nine_imgs_as_vectors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to make labels for the classes formed above. Lables are just 1's and 0's for the two different classes we're classifying by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12665-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# labels of One vs One vectors -- zero && others\n",
    "train_labels_zero_one_class = vcat(zeros(n_zero_train), ones(n_one_train));\n",
    "# train_labels_zero_two_class = vcat(zeros(n_zero_train), ones(n_two_train))\n",
    "# train_labels_zero_three_class = vcat(zeros(n_zero_train), ones(n_three_train))\n",
    "# train_labels_zero_four_class = vcat(zeros(n_zero_train), ones(n_four_train))\n",
    "# train_labels_zero_five_class = vcat(zeros(n_zero_train), ones(n_five_train))\n",
    "# train_labels_zero_six_class = vcat(zeros(n_zero_train), ones(n_six_train))\n",
    "# train_labels_zero_seven_class = vcat(zeros(n_zero_train), ones(n_seven_train))\n",
    "# train_labels_zero_eight_class = vcat(zeros(n_zero_train), ones(n_eight_train))\n",
    "# train_labels_zero_nine_class = vcat(zeros(n_zero_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- one && others\n",
    "# train_labels_one_two_class = vcat(zeros(n_one_train), ones(n_two_train))\n",
    "# train_labels_one_three_class = vcat(zeros(n_one_train), ones(n_three_train))\n",
    "# train_labels_one_four_class = vcat(zeros(n_one_train), ones(n_four_train))\n",
    "# train_labels_one_five_class = vcat(zeros(n_one_train), ones(n_five_train))\n",
    "# train_labels_one_six_class = vcat(zeros(n_one_train), ones(n_six_train))\n",
    "# train_labels_one_seven_class = vcat(zeros(n_one_train), ones(n_seven_train))\n",
    "# train_labels_one_eight_class = vcat(zeros(n_one_train), ones(n_eight_train))\n",
    "# train_labels_one_nine_class = vcat(zeros(n_one_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- two && others\n",
    "# train_labels_two_three_class = vcat(zeros(n_two_train), ones(n_three_train))\n",
    "# train_labels_two_four_class = vcat(zeros(n_two_train), ones(n_four_train))\n",
    "# train_labels_two_five_class = vcat(zeros(n_two_train), ones(n_five_train))\n",
    "# train_labels_two_six_class = vcat(zeros(n_two_train), ones(n_six_train))\n",
    "# train_labels_two_seven_class = vcat(zeros(n_two_train), ones(n_seven_train))\n",
    "# train_labels_two_eight_class = vcat(zeros(n_two_train), ones(n_eight_train))\n",
    "# train_labels_two_nine_class = vcat(zeros(n_two_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- three && others\n",
    "# train_labels_three_four_class = vcat(zeros(n_three_train), ones(n_four_train))\n",
    "# train_labels_three_five_class = vcat(zeros(n_three_train), ones(n_five_train))\n",
    "# train_labels_three_six_class = vcat(zeros(n_three_train), ones(n_six_train))\n",
    "# train_labels_three_seven_class = vcat(zeros(n_three_train), ones(n_seven_train))\n",
    "# train_labels_three_eight_class = vcat(zeros(n_three_train), ones(n_eight_train))\n",
    "# train_labels_three_nine_class = vcat(zeros(n_three_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- four && others\n",
    "# train_labels_four_five_class = vcat(zeros(n_four_train), ones(n_five_train))\n",
    "# train_labels_four_six_class = vcat(zeros(n_four_train), ones(n_six_train))\n",
    "# train_labels_four_seven_class = vcat(zeros(n_four_train), ones(n_seven_train))\n",
    "# train_labels_four_eight_class = vcat(zeros(n_four_train), ones(n_eight_train))\n",
    "# train_labels_four_nine_class = vcat(zeros(n_four_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- five && others\n",
    "# train_labels_five_six_class = vcat(zeros(n_five_train), ones(n_six_train))\n",
    "# train_labels_five_seven_class = vcat(zeros(n_five_train), ones(n_seven_train))\n",
    "# train_labels_five_eight_class = vcat(zeros(n_five_train), ones(n_eight_train))\n",
    "# train_labels_five_nine_class = vcat(zeros(n_five_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- six && others\n",
    "# train_labels_six_seven_class = vcat(zeros(n_six_train), ones(n_seven_train))\n",
    "# train_labels_six_eight_class = vcat(zeros(n_six_train), ones(n_eight_train))\n",
    "# train_labels_six_nine_class = vcat(zeros(n_six_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- seven && others\n",
    "# train_labels_seven_eight_class = vcat(zeros(n_seven_train), ones(n_eight_train))\n",
    "# train_labels_seven_nine_class = vcat(zeros(n_seven_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- eight && others\n",
    "# train_labels_eight_nine_class = vcat(zeros(n_eight_train), ones(n_nine_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.6 sec) Loss = 16.856419978187223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.53 sec) Loss = 0.2889672616554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.53 sec) Loss = 0.18498519307489092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = ovo_train_logistic(train_data_zero_one_class, train_labels_zero_one_class)\n",
    "\n",
    "first_zero_vector = zero_imgs_as_vectors[1,:]\n",
    "first_one_vector = one_imgs_as_vectors[1,:]\n",
    "logistic_predict(first_one_vector, w') # works on test data set, wooo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955947136563876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_one_imgs = MNIST_test_imgs[:,:,MNIST_test_labels .== 1]\n",
    "test_one_imgs_as_vectors = vcat([vec(test_one_imgs[:,:,k])' for k in 1:last(size(test_one_imgs))]...)\n",
    "\n",
    "mean(logistic_classifier(test_one_imgs_as_vectors', w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"T-Shirt\"\n",
       " \"Trouser\"\n",
       " \"Pullover\"\n",
       " \"Dress\"\n",
       " \"Coat\"\n",
       " \"Sandal\"\n",
       " \"Shirt\"\n",
       " \"Sneaker\"\n",
       " \"Bag\"\n",
       " \"Ankle boot\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FashionMNIST.classnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Multi-class Classifier (logistic softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task focusses on logistic softmax regression. From lectures we know that the softmax logistic regression is,\n",
    "\n",
    "$$ \\hat{y} = S_{\\textrm{softmax}}(b + Wx) $$\n",
    "\n",
    "where the softmax function is,\n",
    "\n",
    "$$ S_{\\textrm{softmax}}(z) = \\frac{1}{\\sum_{i=1}^{K}e^{z_{i}}}\\begin{bmatrix} e^{z_{1}} \\\\ \\vdots \\\\ e^{z_{K}} \\end{bmatrix} $$\n",
    "\n",
    "Using Cross Entropy loss once more, this time with Softmax gives us the following,\n",
    "\n",
    "$$ C(w) = -\\sum_{i=1}^{N}ylog{\\hat{y}} $$\n",
    "\n",
    "This means that the gradient is,\n",
    "\n",
    "$$ \\nabla C(w) = (\\hat{y} - y)x $$\n",
    "\n",
    "all functions for logistic softmax regression can be found in `/mnist-classification/logistic-softmax-regression.jl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.74 sec) Loss = 0.8589541136761422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.31 sec) Loss = 0.6638230898520099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.3 sec) Loss = 0.5761267021479102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.3 sec) Loss = 0.5244914012414507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.31 sec) Loss = 0.4894022871270895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.31 sec) Loss = 0.4638410504966134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.53 sec) Loss = 0.44392259529455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.24 sec) Loss = 0.427918929445374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.23 sec) Loss = 0.41460106272722763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.3 sec) Loss = 0.40328080585328735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.24 sec) Loss = 0.39345923777247804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.23 sec) Loss = 0.38481967793027083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.27 sec) Loss = 0.37712721423076984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.25 sec) Loss = 0.3701947356026861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.24 sec) Loss = 0.3638914771135347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.25 sec) Loss = 0.3581172173529608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.24 sec) Loss = 0.35279579206130707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.23 sec) Loss = 0.3478723941946681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.25 sec) Loss = 0.34330007393955564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.25 sec) Loss = 0.3390427258044317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 21 (0.26 sec) Loss = 0.33507093128027426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 22 (0.23 sec) Loss = 0.3313571722215263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 23 (0.23 sec) Loss = 0.3278839952228694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 24 (0.24 sec) Loss = 0.32462610014438253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 25 (0.23 sec) Loss = 0.3215727735001224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 26 (0.23 sec) Loss = 0.31870348303797486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 27 (0.29 sec) Loss = 0.31599300770106326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 28 (0.23 sec) Loss = 0.3134836825706334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 29 (0.24 sec) Loss = 0.3109733178153201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 30 (0.25 sec) Loss = 0.30906871156945015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 31 (0.24 sec) Loss = 0.30614284051106055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 32 (0.27 sec) Loss = 0.30989047088123395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 33 (0.23 sec) Loss = 0.3029336486181426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 34 (0.23 sec) Loss = 0.303547574712528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 35 (0.25 sec) Loss = 0.29887335637868795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 36 (0.23 sec) Loss = 0.30205192572694556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 37 (0.25 sec) Loss = 0.29618326345100765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 38 (0.27 sec) Loss = 0.29996993410544964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 39 (0.23 sec) Loss = 0.2936528126021526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 40 (0.23 sec) Loss = 0.29311215940762153\n"
     ]
    }
   ],
   "source": [
    "W = train_softmax_logistic(η=0.0025, n_epochs=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9168"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNIST_softmax_acc = mean([logistic_sofmax_classifier(X_test'[:,k], W) for k in 1:n_test] .== test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.24 sec) Loss = 2.4676640377588863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.27 sec) Loss = 1.9331564143777646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.24 sec) Loss = 1.6817431979172146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.29 sec) Loss = 1.524183000184793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.31 sec) Loss = 1.413533981299424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.26 sec) Loss = 1.3296400246016409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.23 sec) Loss = 1.2631365253224183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.24 sec) Loss = 1.2092487160781569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.43 sec) Loss = 1.1644408422344374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.31 sec) Loss = 1.1261244137782562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.3 sec) Loss = 1.0926217423197022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.3 sec) Loss = 1.0628664191812471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.51 sec) Loss = 1.0361536974152157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.23 sec) Loss = 1.0119877995548272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.25 sec) Loss = 0.9900017545255059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.24 sec) Loss = 0.9699147815681457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.24 sec) Loss = 0.9515052940674795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.24 sec) Loss = 0.9345916059591892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.24 sec) Loss = 0.9190184420145636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.25 sec) Loss = 0.9046483238404551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 21 (0.24 sec) Loss = 0.8913569795074036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 22 (0.25 sec) Loss = 0.8790315937653499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 23 (0.24 sec) Loss = 0.8675703557673793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 24 (0.24 sec) Loss = 0.856882192807544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 25 (0.24 sec) Loss = 0.8468863071980656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 26 (0.23 sec) Loss = 0.8375115059685334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 27 (0.23 sec) Loss = 0.8286953900169659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 28 (0.24 sec) Loss = 0.8203834707941748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 29 (0.24 sec) Loss = 0.8125282726128032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 30 (0.23 sec) Loss = 0.805088462500861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 31 (0.24 sec) Loss = 0.798028034308544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 32 (0.24 sec) Loss = 0.7913155625527034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 33 (0.23 sec) Loss = 0.7849235329501144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 34 (0.24 sec) Loss = 0.7788277499778988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 35 (0.24 sec) Loss = 0.7730068176418184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 36 (0.24 sec) Loss = 0.7674416883115251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 37 (0.23 sec) Loss = 0.762115275233188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 38 (0.23 sec) Loss = 0.7570121255333062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 39 (0.24 sec) Loss = 0.7521181507835638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 40 (0.25 sec) Loss = 0.7474204111495663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 41 (0.24 sec) Loss = 0.7429069474136576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 42 (0.23 sec) Loss = 0.7385666536412148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 43 (0.24 sec) Loss = 0.7343891825121257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 44 (0.23 sec) Loss = 0.7303648754373882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 45 (0.24 sec) Loss = 0.7264847103060147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 46 (0.24 sec) Loss = 0.7227402607920265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 47 (0.24 sec) Loss = 0.7191236623968102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 48 (0.23 sec) Loss = 0.7156275816814405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 49 (0.23 sec) Loss = 0.7122451863654962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 50 (0.24 sec) Loss = 0.7089701150555558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 51 (0.23 sec) Loss = 0.7057964462534779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 52 (0.24 sec) Loss = 0.7027186669393959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 53 (0.24 sec) Loss = 0.6997316414137242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 54 (0.23 sec) Loss = 0.6968305812327164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 55 (0.23 sec) Loss = 0.6940110170243091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 56 (0.24 sec) Loss = 0.6912687727833008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 57 (0.23 sec) Loss = 0.6885999429824827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 58 (0.23 sec) Loss = 0.6860008725613722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 59 (0.23 sec) Loss = 0.6834681396166055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 60 (0.23 sec) Loss = 0.6809985404495467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 61 (0.24 sec) Loss = 0.6785890765386149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 62 (0.23 sec) Loss = 0.6762369429902043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 63 (0.24 sec) Loss = 0.673939518064852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 64 (0.24 sec) Loss = 0.6716943534518875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 65 (0.23 sec) Loss = 0.6694991650544848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 66 (0.24 sec) Loss = 0.6673518241313837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 67 (0.23 sec) Loss = 0.6652503487114023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 68 (0.24 sec) Loss = 0.6631928952480283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 69 (0.23 sec) Loss = 0.6611777505141823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 70 (0.25 sec) Loss = 0.6592033237545071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 71 (0.24 sec) Loss = 0.6572681391183237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 72 (0.23 sec) Loss = 0.6553708283946102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 73 (0.23 sec) Loss = 0.6535101240644632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 74 (0.23 sec) Loss = 0.6516848526789935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 75 (0.23 sec) Loss = 0.6498939285631921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 76 (0.22 sec) Loss = 0.6481363478399743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 77 (0.17 sec) Loss = 0.6464111827637927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 78 (0.17 sec) Loss = 0.644717576350015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 79 (0.17 sec) Loss = 0.6430547372844623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 80 (0.17 sec) Loss = 0.6414219350969341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 81 (0.16 sec) Loss = 0.6398184955828135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 82 (0.17 sec) Loss = 0.6382437964577062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 83 (0.17 sec) Loss = 0.6366972632312936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 84 (0.17 sec) Loss = 0.6351783652879043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 85 (0.17 sec) Loss = 0.6336866121626853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 86 (0.17 sec) Loss = 0.6322215500035219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 87 (0.17 sec) Loss = 0.6307827582099882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 88 (0.17 sec) Loss = 0.6293698462416081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 89 (0.17 sec) Loss = 0.6279824505885252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 90 (0.17 sec) Loss = 0.626620231898374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 91 (0.17 sec) Loss = 0.6252828722537084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 92 (0.17 sec) Loss = 0.6239700725947827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 93 (0.17 sec) Loss = 0.6226815502828522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 94 (0.17 sec) Loss = 0.6214170367994222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 95 (0.17 sec) Loss = 0.6201762755770908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 96 (0.17 sec) Loss = 0.6189590199577646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 97 (0.18 sec) Loss = 0.6177650312741355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 98 (0.17 sec) Loss = 0.6165940770503876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 99 (0.17 sec) Loss = 0.6154459293181461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100 (0.17 sec) Loss = 0.6143203630437892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8079"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = train_softmax_logistic(n_epochs = 100, η=0.00029)\n",
    "fMNIST_softmax_acc = mean([logistic_sofmax_classifier(X_test'[:,k], W) for k in 1:n_test] .== test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracy after explicitly solving for the gradient function in the gradient descent training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_softmax_acc = 0.9168\n",
      "fMNIST_softmax_acc = 0.8079\n"
     ]
    }
   ],
   "source": [
    "@show MNIST_softmax_acc\n",
    "@show fMNIST_softmax_acc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Comparison of results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking regression models!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for MNIST linear regression model is:   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.078 s (369998 allocations: 2.11 GiB)\n",
      "\n",
      "Runtime for Fashion MNIST linear regression model is: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.000 s (369997 allocations: 2.11 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runtime for MNIST logistic regression model is:   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.046 s (30064 allocations: 7.42 GiB)\n",
      "\n",
      "Runtime for Fashion MNIST logistic regression model is: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24.804 s (150304 allocations: 37.11 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runtime for  MNIST logistic softmax regression model is:   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.553 s (74524 allocations: 15.06 GiB)\n",
      "\n",
      "Runtime for  Fashion MNIST logistic softmax regression model is: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24.456 s (186304 allocations: 37.65 GiB)\n"
     ]
    }
   ],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels\n",
    "test_imgs = MNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "print(\"Runtime for MNIST linear regression model is: \")\n",
    "@btime train_linear()\n",
    "\n",
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels\n",
    "test_imgs = fMNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "print(\"\\nRuntime for Fashion MNIST linear regression model is: \")\n",
    "@btime train_linear()\n",
    "\n",
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "train_labels = MNIST_train_labels;\n",
    "\n",
    "print(\"\\nRuntime for MNIST logistic regression model is: \")\n",
    "@btime train_logistic(η=0.001, verbose=false);\n",
    "\n",
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels = fMNIST_train_labels;\n",
    "\n",
    "print(\"\\nRuntime for Fashion MNIST logistic regression model is: \")\n",
    "@btime train_logistic(η=0.00037, n_epochs=100, verbose=false);\n",
    "\n",
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels;\n",
    "\n",
    "print(\"\\nRuntime for  MNIST logistic softmax regression model is: \")\n",
    "@btime train_softmax_logistic(η=0.0025, n_epochs=40, verbose=false)\n",
    "\n",
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels;\n",
    "\n",
    "print(\"\\nRuntime for  Fashion MNIST logistic softmax regression model is: \")\n",
    "@btime train_softmax_logistic(n_epochs = 100, η=0.00037, verbose=false);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All regression models all have 745 model parameters. One vs One has 45 models with each model having 745 model parameters. Thus for One vs One regression, there is 33,525 model parameters being stored. I was unable to complete task 2.2 so I have left the remainder of the table blank as I am unable to comment on the accuracy or time complexity of the One vs One functions.  As there are a substantial amount more (45x more) model parameters however, I would assumed that the accuracy would improve, but the time taken for the models to complete would greatly increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data Source|OvA inear Regression|OvA Logistic Regression|OvO Linear Regression|OvO Logistic Regression|Logistic Softmax Regression|\n",
    "|-----------|--------------------|-----------------------|---------------------|-----------------------|---------------------------|\n",
    "|MNIST Accuracy|0.8603|0.9035|-|-|0.9151\n",
    "|MNIST Complexity (# parameters)|745|745|33525|33525|745\n",
    "|MNIST (approx) Time|4s|5s|-|-|9s \n",
    "|Fashion MNIST Accuracy|0.8113|0.8191||-|0.9012\n",
    "|Fashion MNIST Complexity|745|745|33525|33525|745\n",
    "|Fashion MNIST Time|4s|23.5s|-|-|24s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we didn't carry out a multi-class linear model example, the multi-class linear model is equivalent to the one vs. all approach taken in task 2.1. This is because the one vs. all approach taken is a method in which we split a multi-class data set into multiple binary classification models, and then train each binary classification models. Predicitions are made using the binary model that is the most confident. For example, looking at the MNIST dataset, we are interested in the multi-class classification between 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. This is then separated into 10 different binary classification datasets such as:\n",
    "- 0 vs rest\n",
    "- 1 vs rest\n",
    "$  \\\\ \\quad \\vdots $\n",
    "- 9 vs rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Your own Random Forest Implementation (MNIST and FashionMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All your MNIST data initialisation needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = MNIST_train_labels\n",
    "y_test = MNIST_test_labels\n",
    "\n",
    "n_train, n_test = last(size(y_train)), last(size(y_test))\n",
    "\n",
    "X_train = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...)\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading up on Random Forest algorithms and the `DecisionTree.jl` docs gives the following tree model. The parameters are set via experimenting with the values to achieve $>0.92$ accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             100\n",
       "n_subfeatures:       50\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             nothing\n",
       "ensemble:            nothing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNIST_tree_model = RandomForestClassifier(n_subfeatures = 50, n_trees = 100, partial_sampling=0.9, max_depth=-1, rng=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             100\n",
       "n_subfeatures:       50\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "ensemble:            Ensemble of Decision Trees\n",
       "Trees:      100\n",
       "Avg Leaves: 3707.51\n",
       "Avg Depth:  22.72"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTree.fit!(MNIST_tree_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the test labels based on the model fitted above and then calculating the accuracy of the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction accuracy (measured on test set of size 10000): 0.9685\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = DecisionTree.predict(MNIST_tree_model, X_test)\n",
    "MNIST_rf_accuracy = mean(predicted_labels .== y_test)\n",
    "println(\"\\nPrediction accuracy (measured on test set of size $n_test): \", MNIST_rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at our Fashion MNIST dataset doing exactly the same as above, but tweaking the parameters to achieve the desired accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = fMNIST_train_labels\n",
    "y_test = fMNIST_test_labels\n",
    "\n",
    "n_train, n_test = last(size(y_train)), last(size(y_test))\n",
    "\n",
    "X_train = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...)\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             120\n",
       "n_subfeatures:       90\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             nothing\n",
       "ensemble:            nothing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fMNIST_tree_model = RandomForestClassifier(n_subfeatures = 90, n_trees = 120, partial_sampling=0.9, max_depth=-1, rng=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             120\n",
       "n_subfeatures:       90\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "ensemble:            Ensemble of Decision Trees\n",
       "Trees:      120\n",
       "Avg Leaves: 3425.225\n",
       "Avg Depth:  25.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTree.fit!(fMNIST_tree_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction accuracy (measured on test set of size 10000): 0.8811\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = DecisionTree.predict(fMNIST_tree_model, X_test)\n",
    "fMNIST_rf_accuracy = mean(predicted_labels .== y_test)\n",
    "println(\"\\nPrediction accuracy (measured on test set of size $n_test): \", fMNIST_rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around with the parameters only seemed to get me to an accuracy of about 0.88, with neglible gains with a significant time penalty so instead of waiting 5+ minutes to see a 0.0001 improvement in accuracy, I will call it here.  Thus, we have a an accuracy for the MNIST dataset is 0.9685 and Fashion MNIST dataset is 0.8811"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
