{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH2504 Semester 2, 2022 -  Project 3\n",
    "\n",
    "## Task 2 - Basic ML on MNIST and FashionMNIST\n",
    "\n",
    "Student name : Brandon Lowe <br />\n",
    "Student ID : 43162950 <br />\n",
    "<p><a href=\"https://github.com/Jaanlo/Brandon-Lowe-2504-2022-PROJECT3\">GitHub Repo</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/University/MATH2504/project-3/Brandon-Lowe-2504-2022-PROJECT3`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "include(\"src/mnist-classification/dependencies.jl\"); # dependencies for task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 One vs. all (rest) Linear and Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple linear regression model is given by\n",
    "\n",
    "$$ \\hat{y} = \\alpha + \\beta x $$\n",
    "\n",
    "Extending this to the MNIST dataset, the formula can be considered as,\n",
    "\n",
    "$$ \\hat{y}^{(i)} = \\beta_0 + \\sum_{j=1}^{784}\\beta_j x_j^{(i)} $$\n",
    "\n",
    "We'd like to find a \"good\" estimate $ \\beta \\in \\mathbb{R}^{785} $ by minimizing the quadratic loss. We form a $ n \\times p $ matrix $ A $ called a **design matrix** such that \n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} &  \\ldots & x_p^{(1)} \\\\ \n",
    "1 & x_1^{(2)} & x_2^{(2)} &  \\ldots & x_p^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n",
    "1 & x_1^{(n)} & x_2^{(n)} &  \\ldots & x_p^{(n)} \\end{bmatrix} $$\n",
    "\n",
    "Using this notation, we can now express the linear model via\n",
    "\n",
    "$$ \\hat{y} = A\\hat{\\theta} $$\n",
    "\n",
    "First, lets add some variables for ease of use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels\n",
    "test_imgs = MNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for linear regression our loss function can be represented as,\n",
    "\n",
    "$$ L(\\theta) = ||y - A\\theta||^2 = (y - A\\theta)^\\top (y - A\\theta) $$\n",
    "\n",
    "with the gradient being,\n",
    "\n",
    "$$ \\nabla L(\\theta) = -2A^\\top y + 2A^\\top A\\theta $$\n",
    "\n",
    "which has the unique solution when the matrix $ A^\\top A $ is invertible, which we know here that A is\n",
    "\n",
    "$$ \\hat{\\theta} = A^\\dag y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefKeywordError",
     "evalue": "UndefKeywordError: keyword argument A not assigned",
     "output_type": "error",
     "traceback": [
      "UndefKeywordError: keyword argument A not assigned\n",
      "\n",
      "Stacktrace:\n",
      " [1] train_linear()\n",
      "   @ Main ~/Documents/University/MATH2504/project-3/Brandon-Lowe-2504-2022-PROJECT3/src/mnist-classification/linear-regression.jl:3\n",
      " [2] top-level scope\n",
      "   @ ~/Documents/University/MATH2504/project-3/Brandon-Lowe-2504-2022-PROJECT3/MNISTClassification.ipynb:1"
     ]
    }
   ],
   "source": [
    "MNIST_linear_acc = train_linear()\n",
    "println(\"Accuracy of model for MNIST test images: \", MNIST_linear_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking now at the Fashion MNIST dataset, we use a similar approach to above for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"T-Shirt\"\n",
       " \"Trouser\"\n",
       " \"Pullover\"\n",
       " \"Dress\"\n",
       " \"Coat\"\n",
       " \"Sandal\"\n",
       " \"Shirt\"\n",
       " \"Sneaker\"\n",
       " \"Bag\"\n",
       " \"Ankle boot\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = FashionMNIST.classnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know we have 10 classes in this dataset, similar to the MNIST dataset. Moving ahead with the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels\n",
    "test_imgs = fMNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model for MNIST test images: 0.8113\n"
     ]
    }
   ],
   "source": [
    "fMNIST_linear_acc = train_linear()\n",
    "println(\"Accuracy of model for MNIST test images: \", fMNIST_linear_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the logistic regression model. From <a href=\"https://deeplearningmath.org/\">The Mathematical Engineering of Deep Learning</a> Chapter 3, we are given that the logistic regression model can be represented as, \n",
    "\n",
    "$$ \\hat{y} = \\sigma (b + w^\\top x) \\quad \\textrm{where,} \\\\\n",
    "\\sigma(z) =\\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim minimize the cross-entropy loss which is given by,\n",
    "\n",
    "$$ \n",
    "L(w) = -\\sum_{i=1}^{N}(y^{(i)}\\log{\\hat{y}^{(i)}} + (1- y^{(i)})\\log{(1-\\hat{y}^{(i)})}) \\\\\n",
    "\\nabla L(w) = X^\\top (\\hat{Y} - Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out a function using gradient descent specific for training logistic regression models. I chose to use the loss function from Flux.jl for simplicity. Explicit gradient term has been programmed as per project details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the data for MNIST dataset to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "train_labels = MNIST_train_labels;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the train model function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (1.23 sec) Loss = 0.2085357315476932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.28 sec) Loss = 0.15744168722529453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.32 sec) Loss = 0.1366063471037671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.3 sec) Loss = 0.12440842067604052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.24 sec) Loss = 0.11610293517223355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.26 sec) Loss = 0.10997874547508622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.25 sec) Loss = 0.10523106867424734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.25 sec) Loss = 0.10141561082469965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.25 sec) Loss = 0.09826605185033173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.25 sec) Loss = 0.09561256862291026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.26 sec) Loss = 0.09334067468328755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.25 sec) Loss = 0.09136972199085368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.28 sec) Loss = 0.08964089817142187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.25 sec) Loss = 0.08811012253331892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.24 sec) Loss = 0.08674361141088945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.25 sec) Loss = 0.08551497880329456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.25 sec) Loss = 0.08440327734954925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.24 sec) Loss = 0.08339164331231777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.25 sec) Loss = 0.08246633905373343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.25 sec) Loss = 0.0816160606868035\n"
     ]
    }
   ],
   "source": [
    "w = train_logistic(η=0.001);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = [ones(n_test) X_test]\n",
    "MNIST_logistic_acc = mean([logistic_classify(T'[:, k], w) for k in 1:n_test] .== MNIST_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the data for the Fashion MNIST dataset to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels = fMNIST_train_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.35 sec) Loss = 0.3481510752147025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.34 sec) Loss = 0.274571224165549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.34 sec) Loss = 0.2415321441444385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.5 sec) Loss = 0.22163432203031005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.28 sec) Loss = 0.20764577476459456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.26 sec) Loss = 0.19688177367035947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.31 sec) Loss = 0.18821392803484874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.26 sec) Loss = 0.18104585754514801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.27 sec) Loss = 0.17500293269528266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.27 sec) Loss = 0.16983014505449692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.26 sec) Loss = 0.16534788601275996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.26 sec) Loss = 0.1614246712620103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.27 sec) Loss = 0.15796040843200762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.27 sec) Loss = 0.1548770105100023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.26 sec) Loss = 0.15211280471507488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.26 sec) Loss = 0.14961864643653713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.27 sec) Loss = 0.14735498443325878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.26 sec) Loss = 0.1452896447047561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.27 sec) Loss = 0.1433961821916812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.27 sec) Loss = 0.14165265622529172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 21 (0.26 sec) Loss = 0.1400407112982488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 22 (0.26 sec) Loss = 0.13854487726161907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 23 (0.27 sec) Loss = 0.13715202947073515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 24 (0.27 sec) Loss = 0.1358509676717609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 25 (0.26 sec) Loss = 0.13463208458502762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 26 (0.27 sec) Loss = 0.13348710318962217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 27 (0.26 sec) Loss = 0.132408867201362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 28 (0.27 sec) Loss = 0.13139117309365117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 29 (0.28 sec) Loss = 0.13042863480554384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 30 (0.26 sec) Loss = 0.1295165742817189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 31 (0.27 sec) Loss = 0.12865093234449068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 32 (0.27 sec) Loss = 0.1278281951846645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 33 (0.25 sec) Loss = 0.12704533206246224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 34 (0.26 sec) Loss = 0.12629973968526295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 35 (0.28 sec) Loss = 0.1255891880148511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 36 (0.26 sec) Loss = 0.12491176007430617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 37 (0.26 sec) Loss = 0.12426577210490879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 38 (0.28 sec) Loss = 0.12364964575226937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 39 (0.29 sec) Loss = 0.12306168395040988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 40 (0.29 sec) Loss = 0.12249972425536937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 41 (0.28 sec) Loss = 0.12196082048281731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 42 (0.26 sec) Loss = 0.1214413796243753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 43 (0.28 sec) Loss = 0.12093794603006781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 44 (0.27 sec) Loss = 0.12044779211850862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 45 (0.26 sec) Loss = 0.11996793042017302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 46 (0.28 sec) Loss = 0.11949215647072985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 47 (0.26 sec) Loss = 0.11901195397646666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 48 (0.26 sec) Loss = 0.11852965496825632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 49 (0.27 sec) Loss = 0.11805456033426294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 50 (0.27 sec) Loss = 0.11759065813718819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 51 (0.26 sec) Loss = 0.11713910050950714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 52 (0.26 sec) Loss = 0.11670052778950277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 53 (0.27 sec) Loss = 0.11627587227130934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 54 (0.27 sec) Loss = 0.11586589608895748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 55 (0.26 sec) Loss = 0.11546935140372724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 56 (0.27 sec) Loss = 0.11508172960858487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 57 (0.26 sec) Loss = 0.11469880087156166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 58 (0.28 sec) Loss = 0.11432016174016185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 59 (0.26 sec) Loss = 0.11394735507620656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 60 (0.27 sec) Loss = 0.11358167499539193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 61 (0.27 sec) Loss = 0.11322382650509244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 62 (0.26 sec) Loss = 0.11287412815300503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 63 (0.28 sec) Loss = 0.11253267365941201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 64 (0.26 sec) Loss = 0.11219941972594405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 65 (0.27 sec) Loss = 0.11187423434538218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 66 (0.26 sec) Loss = 0.11155692649666496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 67 (0.27 sec) Loss = 0.11124726725301483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 68 (0.28 sec) Loss = 0.11094500651259753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 69 (0.27 sec) Loss = 0.11064988647661492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 70 (0.27 sec) Loss = 0.11036165173310175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 71 (0.27 sec) Loss = 0.11008005577463574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 72 (0.26 sec) Loss = 0.10980486429875284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 73 (0.26 sec) Loss = 0.10953585610786501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 74 (0.27 sec) Loss = 0.10927282256239598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 75 (0.27 sec) Loss = 0.10901556638470626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 76 (0.26 sec) Loss = 0.10876390033381553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 77 (0.27 sec) Loss = 0.1085176460147921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 78 (0.26 sec) Loss = 0.10827663291245584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 79 (0.26 sec) Loss = 0.1080406976461151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 80 (0.28 sec) Loss = 0.10780968340505706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 81 (0.26 sec) Loss = 0.1075834395180066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 82 (0.27 sec) Loss = 0.10736182111609203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 83 (0.27 sec) Loss = 0.10714468885868908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 84 (0.27 sec) Loss = 0.10693190870055094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 85 (0.28 sec) Loss = 0.10672335168574898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 86 (0.26 sec) Loss = 0.1065188937590126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 87 (0.29 sec) Loss = 0.10631841558854013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 88 (0.26 sec) Loss = 0.10612180239664284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 89 (0.26 sec) Loss = 0.10592894379607257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 90 (0.27 sec) Loss = 0.10573973363083086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 91 (0.27 sec) Loss = 0.10555406982083058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 92 (0.27 sec) Loss = 0.10537185421012174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 93 (0.26 sec) Loss = 0.10519299241859319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 94 (0.26 sec) Loss = 0.10501739369712712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 95 (0.26 sec) Loss = 0.10484497078620779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 96 (0.26 sec) Loss = 0.10467563977796902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 97 (0.27 sec) Loss = 0.10450931998161149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 98 (0.26 sec) Loss = 0.10434593379207079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 99 (0.27 sec) Loss = 0.1041854065617507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100 (0.26 sec) Loss = 0.10402766647508396\n"
     ]
    }
   ],
   "source": [
    "w = train_logistic(η=0.00037, n_epochs=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the accuracy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8139"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = [ones(n_test) X_test]\n",
    "fMNIST_logistic_acc = mean([logistic_classify(T'[:, k], w) for k in 1:n_test] .== fMNIST_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So overall, we have the following accuracy results for linear regression and logistic regression for MNIST and Fashion MNIST datasets, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST linear regression model: 0.8603\n",
      "Accuracy for Fashion MNIST linear regression model: 0.8113\n",
      "Accuracy for MNIST logistic regression model: 0.9035\n",
      "Accuracy for Fashion  MNIST logistic regression model: 0.8139\n"
     ]
    }
   ],
   "source": [
    "println(\"Accuracy for MNIST linear regression model: $MNIST_linear_acc\")\n",
    "println(\"Accuracy for Fashion MNIST linear regression model: $fMNIST_linear_acc\")\n",
    "println(\"Accuracy for MNIST logistic regression model: $MNIST_logistic_acc\")\n",
    "println(\"Accuracy for Fashion  MNIST logistic regression model: $fMNIST_logistic_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 One vs. One Linear and Logisitic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 is unfinished in current state. Currently I was experimenting with only one of the 45 different models. It works (at least I think so) for that one model, but I have yet to figure out a clean (and not RAM hungry) method of building all 45 models. Every time I try my IDE crashes -_-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28×28×6742 Array{Float64, 3}:\n",
       "[:, :, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.25098   0.0941176  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.984314  0.756863   0.0  0.0  0.0\n",
       " ⋮                        ⋮         ⋱                       ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.992157     0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  1.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.247059     0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0       0.0        0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 3] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 6740] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.65098  0.65098  0.0  0.0  0.0\n",
       " ⋮                        ⋮         ⋱                    ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.996078     0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.462745  …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0      0.0      0.0  0.0  0.0\n",
       "\n",
       "[:, :, 6741] =\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0        …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0        …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0509804  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.992157   0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.658824   0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0196078  0.0  0.0  0.0  0.0\n",
       " ⋮                             ⋮          ⋱                  ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0       0.0196078     0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.643137  0.882353   …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.996078  0.992157      0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.556863  0.529412      0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0        …  0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0       0.0           0.0        0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 6742] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0156863  0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.788235   0.666667  0.0  0.0  0.0\n",
       " ⋮                        ⋮         ⋱                       ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.215686  …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.901961     0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.32549      0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0       …  0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0          0.0        0.0       0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialising img training sets\n",
    "zero_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 0]\n",
    "one_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 1]\n",
    "# two_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 2]\n",
    "# three_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 3]\n",
    "# four_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 4]\n",
    "# five_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 5]\n",
    "# six_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 6]\n",
    "# seven_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 7]\n",
    "# eight_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 8]\n",
    "# nine_imgs = MNIST_train_imgs[:,:,MNIST_train_labels .== 9];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialising img training set counts\n",
    "n_zero_train = last(size(zero_imgs))\n",
    "n_one_train = last(size(one_imgs))\n",
    "# n_two_train = last(size(two_imgs))\n",
    "# n_three_train = last(size(three_imgs))\n",
    "# n_four_train = last(size(four_imgs))\n",
    "# n_five_train = last(size(five_imgs))\n",
    "# n_six_train = last(size(six_imgs))\n",
    "# n_seven_train = last(size(seven_imgs))\n",
    "# n_eight_train = last(size(eight_imgs))\n",
    "# n_nine_train = last(size(nine_imgs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6742×784 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                 ⋮              \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialising img as vectors\n",
    "zero_imgs_as_vectors = vcat([vec(zero_imgs[:,:,k])' for k in 1:n_zero_train]...)\n",
    "one_imgs_as_vectors = vcat([vec(one_imgs[:,:,k])' for k in 1:n_one_train]...)\n",
    "# two_imgs_as_vectors = vcat([vec(two_imgs[:,:,k])' for k in 1:n_two_train]...)\n",
    "# three_imgs_as_vectors = vcat([vec(three_imgs[:,:,k])' for k in 1:n_three_train]...)\n",
    "# four_imgs_as_vectors = vcat([vec(four_imgs[:,:,k])' for k in 1:n_four_train]...)\n",
    "# five_imgs_as_vectors = vcat([vec(five_imgs[:,:,k])' for k in 1:n_five_train]...)\n",
    "# six_imgs_as_vectors = vcat([vec(six_imgs[:,:,k])' for k in 1:n_six_train]...)\n",
    "# seven_imgs_as_vectors = vcat([vec(seven_imgs[:,:,k])' for k in 1:n_seven_train]...)\n",
    "# eight_imgs_as_vectors = vcat([vec(eight_imgs[:,:,k])' for k in 1:n_eight_train]...)\n",
    "# nine_imgs_as_vectors = vcat([vec(nine_imgs[:,:,k])' for k in 1:n_nine_train]...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the training data classes to classify by different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12665×784 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                 ⋮              \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vcat of One vs One vectors -- zero && others\n",
    "train_data_zero_one_class = vcat(zero_imgs_as_vectors, one_imgs_as_vectors)\n",
    "# train_data_zero_two_class = vcat(zero_imgs_as_vectors, two_imgs_as_vectors)\n",
    "# train_data_zero_three_class = vcat(zero_imgs_as_vectors, three_imgs_as_vectors)\n",
    "# train_data_zero_four_class = vcat(zero_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_zero_five_class = vcat(zero_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_zero_six_class = vcat(zero_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_zero_seven_class = vcat(zero_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_zero_eight_class = vcat(zero_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_zero_nine_class = vcat(zero_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- one && others\n",
    "# train_data_one_two_class = vcat(one_imgs_as_vectors, two_imgs_as_vectors)\n",
    "# train_data_one_three_class = vcat(one_imgs_as_vectors, three_imgs_as_vectors)\n",
    "# train_data_one_four_class = vcat(one_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_one_five_class = vcat(one_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_one_six_class = vcat(one_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_one_seven_class = vcat(one_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_one_eight_class = vcat(one_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_one_nine_class = vcat(one_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- two && others\n",
    "# train_data_two_three_class = vcat(two_imgs_as_vectors, three_imgs_as_vectors)\n",
    "# train_data_two_four_class = vcat(two_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_two_five_class = vcat(two_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_two_six_class = vcat(two_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_two_seven_class = vcat(two_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_two_eight_class = vcat(two_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_two_nine_class = vcat(two_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- three && others\n",
    "# train_data_three_four_class = vcat(three_imgs_as_vectors, four_imgs_as_vectors)\n",
    "# train_data_three_five_class = vcat(three_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_three_six_class = vcat(three_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_three_seven_class = vcat(three_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_three_eight_class = vcat(three_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_three_nine_class = vcat(three_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- four && others\n",
    "# train_data_four_five_class = vcat(four_imgs_as_vectors, five_imgs_as_vectors)\n",
    "# train_data_four_six_class = vcat(four_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_four_seven_class = vcat(four_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_four_eight_class = vcat(four_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_four_nine_class = vcat(four_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- five && others\n",
    "# train_data_five_six_class = vcat(five_imgs_as_vectors, six_imgs_as_vectors)\n",
    "# train_data_five_seven_class = vcat(five_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_five_eight_class = vcat(five_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_five_nine_class = vcat(five_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- six && others\n",
    "# train_data_six_seven_class = vcat(six_imgs_as_vectors, seven_imgs_as_vectors)\n",
    "# train_data_six_eight_class = vcat(six_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_six_nine_class = vcat(six_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- seven && others\n",
    "# train_data_seven_eight_class = vcat(seven_imgs_as_vectors, eight_imgs_as_vectors)\n",
    "# train_data_seven_nine_class = vcat(seven_imgs_as_vectors, nine_imgs_as_vectors)\n",
    "\n",
    "# # vcat of One vs One vectors -- eight && others\n",
    "# train_data_eight_nine_class = vcat(eight_imgs_as_vectors, nine_imgs_as_vectors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to make labels for the classes formed above. Lables are just 1's and 0's for the two different classes we're classifying by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12665-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# labels of One vs One vectors -- zero && others\n",
    "train_labels_zero_one_class = vcat(zeros(n_zero_train), ones(n_one_train))\n",
    "# train_labels_zero_two_class = vcat(zeros(n_zero_train), ones(n_two_train))\n",
    "# train_labels_zero_three_class = vcat(zeros(n_zero_train), ones(n_three_train))\n",
    "# train_labels_zero_four_class = vcat(zeros(n_zero_train), ones(n_four_train))\n",
    "# train_labels_zero_five_class = vcat(zeros(n_zero_train), ones(n_five_train))\n",
    "# train_labels_zero_six_class = vcat(zeros(n_zero_train), ones(n_six_train))\n",
    "# train_labels_zero_seven_class = vcat(zeros(n_zero_train), ones(n_seven_train))\n",
    "# train_labels_zero_eight_class = vcat(zeros(n_zero_train), ones(n_eight_train))\n",
    "# train_labels_zero_nine_class = vcat(zeros(n_zero_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- one && others\n",
    "# train_labels_one_two_class = vcat(zeros(n_one_train), ones(n_two_train))\n",
    "# train_labels_one_three_class = vcat(zeros(n_one_train), ones(n_three_train))\n",
    "# train_labels_one_four_class = vcat(zeros(n_one_train), ones(n_four_train))\n",
    "# train_labels_one_five_class = vcat(zeros(n_one_train), ones(n_five_train))\n",
    "# train_labels_one_six_class = vcat(zeros(n_one_train), ones(n_six_train))\n",
    "# train_labels_one_seven_class = vcat(zeros(n_one_train), ones(n_seven_train))\n",
    "# train_labels_one_eight_class = vcat(zeros(n_one_train), ones(n_eight_train))\n",
    "# train_labels_one_nine_class = vcat(zeros(n_one_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- two && others\n",
    "# train_labels_two_three_class = vcat(zeros(n_two_train), ones(n_three_train))\n",
    "# train_labels_two_four_class = vcat(zeros(n_two_train), ones(n_four_train))\n",
    "# train_labels_two_five_class = vcat(zeros(n_two_train), ones(n_five_train))\n",
    "# train_labels_two_six_class = vcat(zeros(n_two_train), ones(n_six_train))\n",
    "# train_labels_two_seven_class = vcat(zeros(n_two_train), ones(n_seven_train))\n",
    "# train_labels_two_eight_class = vcat(zeros(n_two_train), ones(n_eight_train))\n",
    "# train_labels_two_nine_class = vcat(zeros(n_two_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- three && others\n",
    "# train_labels_three_four_class = vcat(zeros(n_three_train), ones(n_four_train))\n",
    "# train_labels_three_five_class = vcat(zeros(n_three_train), ones(n_five_train))\n",
    "# train_labels_three_six_class = vcat(zeros(n_three_train), ones(n_six_train))\n",
    "# train_labels_three_seven_class = vcat(zeros(n_three_train), ones(n_seven_train))\n",
    "# train_labels_three_eight_class = vcat(zeros(n_three_train), ones(n_eight_train))\n",
    "# train_labels_three_nine_class = vcat(zeros(n_three_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- four && others\n",
    "# train_labels_four_five_class = vcat(zeros(n_four_train), ones(n_five_train))\n",
    "# train_labels_four_six_class = vcat(zeros(n_four_train), ones(n_six_train))\n",
    "# train_labels_four_seven_class = vcat(zeros(n_four_train), ones(n_seven_train))\n",
    "# train_labels_four_eight_class = vcat(zeros(n_four_train), ones(n_eight_train))\n",
    "# train_labels_four_nine_class = vcat(zeros(n_four_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- five && others\n",
    "# train_labels_five_six_class = vcat(zeros(n_five_train), ones(n_six_train))\n",
    "# train_labels_five_seven_class = vcat(zeros(n_five_train), ones(n_seven_train))\n",
    "# train_labels_five_eight_class = vcat(zeros(n_five_train), ones(n_eight_train))\n",
    "# train_labels_five_nine_class = vcat(zeros(n_five_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- six && others\n",
    "# train_labels_six_seven_class = vcat(zeros(n_six_train), ones(n_seven_train))\n",
    "# train_labels_six_eight_class = vcat(zeros(n_six_train), ones(n_eight_train))\n",
    "# train_labels_six_nine_class = vcat(zeros(n_six_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- seven && others\n",
    "# train_labels_seven_eight_class = vcat(zeros(n_seven_train), ones(n_eight_train))\n",
    "# train_labels_seven_nine_class = vcat(zeros(n_seven_train), ones(n_nine_train))\n",
    "\n",
    "# # labels of One vs One vectors -- eight && others\n",
    "# train_labels_eight_nine_class = vcat(zeros(n_eight_train), ones(n_nine_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.59 sec) Loss = 12.411933309823237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.55 sec) Loss = 0.2845926047304945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.61 sec) Loss = 0.19067718624904056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = ovo_train_logistic(train_data_zero_one_class, train_labels_zero_one_class)\n",
    "\n",
    "first_zero_vector = zero_imgs_as_vectors[1,:]\n",
    "first_one_vector = one_imgs_as_vectors[1,:]\n",
    "logistic_predict(first_one_vector, w') # works on test data set, wooo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955947136563876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_one_imgs = MNIST_test_imgs[:,:,MNIST_test_labels .== 1]\n",
    "test_one_imgs_as_vectors = vcat([vec(test_one_imgs[:,:,k])' for k in 1:last(size(test_one_imgs))]...)\n",
    "\n",
    "mean(logistic_classifier(test_one_imgs_as_vectors', w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{String}:\n",
       " \"T-Shirt\"\n",
       " \"Trouser\"\n",
       " \"Pullover\"\n",
       " \"Dress\"\n",
       " \"Coat\"\n",
       " \"Sandal\"\n",
       " \"Shirt\"\n",
       " \"Sneaker\"\n",
       " \"Bag\"\n",
       " \"Ankle boot\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FashionMNIST.classnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Multi-class Classifier (logistic softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task focusses on logistic softmax regression. From lectures we know that the softmax logistic regression is,\n",
    "\n",
    "$$ \\hat{y} = S_{\\textrm{softmax}}(b + Wx) $$\n",
    "\n",
    "where the softmax function is,\n",
    "\n",
    "$$ S_{\\textrm{softmax}}(z) = \\frac{1}{\\sum_{i=1}^{K}e^{z_{i}}}\\begin{bmatrix} e^{z_{1}} \\\\ \\vdots \\\\ e^{z_{K}} \\end{bmatrix} $$\n",
    "\n",
    "Using Cross Entropy loss once more, this time with Softmax gives us the following,\n",
    "\n",
    "$$ C(w) = -\\sum_{i=1}^{N}ylog{\\hat{y}} $$\n",
    "\n",
    "This means that the gradient is,\n",
    "\n",
    "$$ \\nabla C(w) = (\\hat{y} - y)x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.9 sec) Loss = 0.817566516941731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.32 sec) Loss = 0.6454115492817132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.6 sec) Loss = 0.567196251676804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.27 sec) Loss = 0.5185601699495995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.28 sec) Loss = 0.48449191377633494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.39 sec) Loss = 0.45903569609659595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.27 sec) Loss = 0.43909239517499055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.27 sec) Loss = 0.4229146017639181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.29 sec) Loss = 0.40946250432088444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.27 sec) Loss = 0.3980612333446103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.27 sec) Loss = 0.3882474683992218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.25 sec) Loss = 0.37969092724595865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.26 sec) Loss = 0.37214813567030824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.25 sec) Loss = 0.3654342829627328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.28 sec) Loss = 0.3594066642890467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.27 sec) Loss = 0.35395373520292356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.25 sec) Loss = 0.34898742414304845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.25 sec) Loss = 0.3444374103396466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.27 sec) Loss = 0.34024695244642356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.25 sec) Loss = 0.3363697774075387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 21 (0.24 sec) Loss = 0.3327678073280357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 22 (0.29 sec) Loss = 0.32940943252854193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 23 (0.28 sec) Loss = 0.32626822781067166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 24 (0.25 sec) Loss = 0.32332193045367863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 25 (0.26 sec) Loss = 0.32055167412537977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 26 (0.25 sec) Loss = 0.3179413261313399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 27 (0.25 sec) Loss = 0.31547699741322477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 28 (0.27 sec) Loss = 0.31314654625260613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 29 (0.31 sec) Loss = 0.31093924462640193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 30 (0.31 sec) Loss = 0.3088454240192396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 31 (0.27 sec) Loss = 0.3068563318353966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 32 (0.29 sec) Loss = 0.3049640824826239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 33 (0.25 sec) Loss = 0.3031616955006112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 34 (0.35 sec) Loss = 0.3014430105081032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 35 (0.28 sec) Loss = 0.2998022940420993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 36 (0.26 sec) Loss = 0.2982337961011619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 37 (0.19 sec) Loss = 0.29673241991375077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 38 (0.21 sec) Loss = 0.29529648934156044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 39 (0.2 sec) Loss = 0.293928370992225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 40 (0.24 sec) Loss = 0.29261589289489115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×785 Matrix{Float64}:\n",
       " -1.34844     0.258306   -2.31237   …  -0.454338  -0.42648     0.195103\n",
       "  1.23631     0.474921    0.481338      0.707621  -0.687472   -0.0645806\n",
       "  1.06314    -1.4522      1.57689      -0.378551   0.27949    -1.41804\n",
       " -0.330774   -1.17276    -1.89043      -2.17402    0.305495   -0.12709\n",
       "  0.583853   -1.03265    -0.44157      -1.1264     0.149201   -0.496329\n",
       "  3.62302     0.84324    -1.50981   …  -0.291167   0.0194851  -0.566644\n",
       " -0.117291   -0.751446   -0.211865     -1.06725    1.61749    -0.280302\n",
       "  2.51693    -0.0694654  -1.18448      -0.159258  -0.782225   -3.24153\n",
       " -3.27556    -0.526376    2.18799       0.606248  -0.40031     0.842567\n",
       "  0.0896521   0.211745   -0.871964      0.508418   0.272255    0.0866512"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = train_softmax_logistic(η=0.0025, n_epochs=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNIST_softmax_acc = mean([logistic_sofmax_classifier(X_test'[:,k], W) for k in 1:n_test] .== test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 (0.31 sec) Loss = 2.5677423165203512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 (0.26 sec) Loss = 1.9650366432150408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 (0.25 sec) Loss = 1.7035654764147752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 (0.25 sec) Loss = 1.5460575636442153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 (0.25 sec) Loss = 1.4364874839238833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 (0.26 sec) Loss = 1.353522820870328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 (0.25 sec) Loss = 1.287218188299178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 (0.25 sec) Loss = 1.2322735341703432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 (0.25 sec) Loss = 1.1856692431603844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10 (0.26 sec) Loss = 1.1454908919736544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11 (0.26 sec) Loss = 1.1104354594035108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12 (0.24 sec) Loss = 1.0795674746573425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13 (0.25 sec) Loss = 1.0521730690691125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14 (0.28 sec) Loss = 1.0276657064312522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15 (0.26 sec) Loss = 1.0055513565234746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16 (0.25 sec) Loss = 0.9854327298244364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17 (0.26 sec) Loss = 0.96700632483562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18 (0.26 sec) Loss = 0.9500393702863975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19 (0.26 sec) Loss = 0.934344785294517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20 (0.25 sec) Loss = 0.9197663156530664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 21 (0.25 sec) Loss = 0.9061721603282031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 22 (0.25 sec) Loss = 0.8934515708388893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 23 (0.26 sec) Loss = 0.8815115988676675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 24 (0.25 sec) Loss = 0.8702738229626513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 25 (0.29 sec) Loss = 0.8596714973642685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 26 (0.26 sec) Loss = 0.8496472748026593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 27 (0.26 sec) Loss = 0.8401514317007684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 28 (0.28 sec) Loss = 0.831140485153845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 29 (0.27 sec) Loss = 0.8225761119518943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 30 (0.28 sec) Loss = 0.8144243004656848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 31 (0.27 sec) Loss = 0.8066546806788164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 32 (0.27 sec) Loss = 0.7992399894602435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 33 (0.27 sec) Loss = 0.792155638545771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 34 (0.27 sec) Loss = 0.7853793617860674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 35 (0.26 sec) Loss = 0.778890925393504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 36 (0.25 sec) Loss = 0.7726718892878648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 37 (0.27 sec) Loss = 0.766705409448509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 38 (0.25 sec) Loss = 0.7609760719503631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 39 (0.26 sec) Loss = 0.755469750514753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 40 (0.26 sec) Loss = 0.7501734812587608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 41 (0.27 sec) Loss = 0.745075350370161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 42 (0.25 sec) Loss = 0.7401643921173507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 43 (0.25 sec) Loss = 0.7354304957206087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 44 (0.25 sec) Loss = 0.7308643202366412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 45 (0.24 sec) Loss = 0.7264572169050618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 46 (0.25 sec) Loss = 0.7222011585159429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 47 (0.25 sec) Loss = 0.7180886753834907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 48 (0.28 sec) Loss = 0.7141127975180365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 49 (0.25 sec) Loss = 0.7102670026099128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 50 (0.26 sec) Loss = 0.7065451694763069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 51 (0.25 sec) Loss = 0.7029415366561964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 52 (0.26 sec) Loss = 0.6994506658457826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 53 (0.25 sec) Loss = 0.69606740983793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 54 (0.27 sec) Loss = 0.6927868845752932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 55 (0.27 sec) Loss = 0.6896044448732261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 56 (0.27 sec) Loss = 0.6865156633398823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 57 (0.25 sec) Loss = 0.6835163120287141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 58 (0.25 sec) Loss = 0.6806023463987126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 59 (0.25 sec) Loss = 0.6777698912157585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 60 (0.24 sec) Loss = 0.6750152280887468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 61 (0.25 sec) Loss = 0.6723347843863969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 62 (0.25 sec) Loss = 0.6697251233216125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 63 (0.25 sec) Loss = 0.6671829350220503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 64 (0.25 sec) Loss = 0.6647050284326049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 65 (0.25 sec) Loss = 0.6622883239216949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 66 (0.24 sec) Loss = 0.6599298464906865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 67 (0.25 sec) Loss = 0.6576267195144253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 68 (0.25 sec) Loss = 0.6553761589690283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 69 (0.26 sec) Loss = 0.653175468128623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 70 (0.25 sec) Loss = 0.6510220327337056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 71 (0.25 sec) Loss = 0.6489133166492846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 72 (0.25 sec) Loss = 0.6468468580411784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 73 (0.25 sec) Loss = 0.6448202661047568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 74 (0.25 sec) Loss = 0.6428312183834655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 75 (0.25 sec) Loss = 0.6408774587157711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 76 (0.25 sec) Loss = 0.6389567958494753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 77 (0.26 sec) Loss = 0.6370671027616321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 78 (0.24 sec) Loss = 0.6352063167201152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 79 (0.26 sec) Loss = 0.6333724401183921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 80 (0.28 sec) Loss = 0.6315635421073001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 81 (0.26 sec) Loss = 0.6297777610356629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 82 (0.26 sec) Loss = 0.6280133076948375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 83 (0.25 sec) Loss = 0.6262684693402304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 84 (0.25 sec) Loss = 0.6245416144355729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 85 (0.25 sec) Loss = 0.6228311980337304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 86 (0.25 sec) Loss = 0.6211357676720957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 87 (0.24 sec) Loss = 0.6194539696227349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 88 (0.25 sec) Loss = 0.6177845552996512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 89 (0.25 sec) Loss = 0.6161263875903685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 90 (0.24 sec) Loss = 0.6144784468497297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 91 (0.25 sec) Loss = 0.6128398362733795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 92 (0.24 sec) Loss = 0.6112097863599738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 93 (0.24 sec) Loss = 0.6095876581773763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 94 (0.24 sec) Loss = 0.6079729451705156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 95 (0.27 sec) Loss = 0.6063652732879083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 96 (0.25 sec) Loss = 0.6047643992591816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 97 (0.25 sec) Loss = 0.60317020692489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 98 (0.25 sec) Loss = 0.6015827015988745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 99 (0.25 sec) Loss = 0.6000020025272952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100 (0.25 sec) Loss = 0.5984283335917892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8094"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = train_softmax_logistic(n_epochs = 100, η=0.00029)\n",
    "fMNIST_softmax_acc = mean([logistic_sofmax_classifier(X_test'[:,k], W) for k in 1:n_test] .== test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracy after explicitly solving for the gradient function in the gradient descent training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_softmax_acc = 0.9149\n",
      "fMNIST_softmax_acc = 0.8094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8094"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show MNIST_softmax_acc\n",
    "@show fMNIST_softmax_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Comparison of results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking regression models!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime for MNIST linear regression model is:   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.012 s (369998 allocations: 2.11 GiB)\n",
      "\n",
      "Runtime for Fashion MNIST linear regression model is: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.083 s (369997 allocations: 2.11 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runtime for MNIST logistic regression model is:   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.002 s (30064 allocations: 7.42 GiB)\n",
      "\n",
      "Runtime for Fashion MNIST logistic regression model is: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23.313 s (150304 allocations: 37.11 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runtime for  MNIST logistic softmax regression model is:   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.809 s (74524 allocations: 15.06 GiB)\n",
      "\n",
      "Runtime for  Fashion MNIST logistic softmax regression model is: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23.858 s (186304 allocations: 37.65 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×785 Matrix{Float64}:\n",
       "  1.17555    -2.00866    -2.41099   …  -1.55601    1.79848    1.03918\n",
       " -2.1953      0.0504965  -0.461909     -0.440409   0.681894  -0.894412\n",
       " -0.0746261  -0.474053   -1.12971      -0.84933   -1.52699   -0.892821\n",
       "  1.16851    -0.507603    0.226753     -0.832529  -0.028379   0.0247106\n",
       " -2.42795    -0.368593   -0.436138     -0.55818    2.03987    0.357803\n",
       "  5.75897    -0.413496    0.166273  …  -0.522101   1.42155   -1.99173\n",
       "  1.14571     1.37329     0.73147      -1.12914    0.273237   2.5101\n",
       "  1.27688     0.959132   -0.448661      0.721781   0.182656  -1.52335\n",
       " -1.29992     0.215909    0.82105      -0.74605   -0.365856  -1.06695\n",
       " -2.46925     2.21365     1.44386       0.341736   1.56239    1.76427"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels\n",
    "test_imgs = MNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "print(\"Runtime for MNIST linear regression model is: \")\n",
    "@btime train_linear()\n",
    "\n",
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels\n",
    "test_imgs = fMNIST_test_imgs\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "print(\"\\nRuntime for Fashion MNIST linear regression model is: \")\n",
    "@btime train_linear()\n",
    "\n",
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X];\n",
    "\n",
    "train_labels = MNIST_train_labels;\n",
    "\n",
    "print(\"\\nRuntime for MNIST logistic regression model is: \")\n",
    "@btime train_logistic(η=0.001, verbose=false);\n",
    "\n",
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels);\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels = fMNIST_train_labels;\n",
    "\n",
    "print(\"\\nRuntime for Fashion MNIST logistic regression model is: \")\n",
    "@btime train_logistic(η=0.00037, n_epochs=100, verbose=false);\n",
    "\n",
    "n_train, n_test = length(MNIST_train_labels), length(MNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = MNIST_train_labels, MNIST_test_labels;\n",
    "\n",
    "print(\"\\nRuntime for  MNIST logistic softmax regression model is: \")\n",
    "@btime train_softmax_logistic(η=0.0025, n_epochs=40, verbose=false)\n",
    "\n",
    "n_train, n_test = length(fMNIST_train_labels), length(fMNIST_test_labels)\n",
    "\n",
    "X = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...);\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);\n",
    "A = [ones(n_train) X]\n",
    "\n",
    "train_labels, test_labels = fMNIST_train_labels, fMNIST_test_labels;\n",
    "\n",
    "print(\"\\nRuntime for  Fashion MNIST logistic softmax regression model is: \")\n",
    "@btime train_softmax_logistic(n_epochs = 100, η=0.00037, verbose=false);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All regression models all have 745 model parameters. One vs One has 45 models with each model having 745 model parameters. Thus for One vs One regression, there is 33,525 model parameters being stored. I was unable to complete task 2.2 so I have left the remainder of the table blank as I am unable to comment on the accuracy or time complexity of the One vs One functions.  As there are a substantial amount more (45x more) model parameters however, I would assumed that the accuracy would improve, but the time taken for the models to complete would greatly increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data Source|OvA inear Regression|OvA Logistic Regression|OvO Linear Regression|OvO Logistic Regression|Logistic Softmax Regression|\n",
    "|-----------|--------------------|-----------------------|---------------------|-----------------------|---------------------------|\n",
    "|MNIST Accuracy|0.8603|0.9035|-|-|0.9151\n",
    "|MNIST Complexity (# parameters)|745|745|33525|33525|745\n",
    "|MNIST Time|4.012s|5.02s|-|-|8.809s \n",
    "|Fashion MNIST Accuracy|0.8113|0.8191||-|0.9012\n",
    "|Fashion MNIST Complexity|745|745|33525|33525|745\n",
    "|Fashion MNIST Time|4.083s|23.313s|-|-|23.858s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we didn't carry out a multi-class linear model example, the multi-class linear model is equivalent to the one vs. all approach taken in task 2.1. This is because the one vs. all approach taken is a method in which we split a multi-class data set into multiple binary classification models, and then train each binary classification models. Predicitions are made using the binary model that is the most confident. For example, looking at the MNIST dataset, we are interested in the multi-class classification between 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. This is then separated into 10 different binary classification datasets such as:\n",
    "- 0 vs rest\n",
    "- 1 vs rest\n",
    "$  \\\\ \\quad \\vdots $\n",
    "- 9 vs rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Your own Random Forest Implementation (MNIST and FashionMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All your MNIST data initialisation needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = MNIST_train_labels\n",
    "y_test = MNIST_test_labels\n",
    "\n",
    "n_train, n_test = last(size(y_train)), last(size(y_test))\n",
    "\n",
    "X_train = vcat([vec(MNIST_train_imgs[:,:,k])' for k in 1:n_train]...)\n",
    "X_test = vcat([vec(MNIST_test_imgs[:,:,k])' for k in 1:n_test]...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading up on Random Forest algorithms and the `DecisionTree.jl` docs gives the following tree model. The parameters are set via experimenting with the values to achieve $>0.92$ accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             100\n",
       "n_subfeatures:       50\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             nothing\n",
       "ensemble:            nothing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNIST_tree_model = RandomForestClassifier(n_subfeatures = 50, n_trees = 100, partial_sampling=0.9, max_depth=-1, rng=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             100\n",
       "n_subfeatures:       50\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "ensemble:            Ensemble of Decision Trees\n",
       "Trees:      100\n",
       "Avg Leaves: 3707.51\n",
       "Avg Depth:  22.72"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTree.fit!(MNIST_tree_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the test labels based on the model fitted above and then calculating the accuracy of the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction accuracy (measured on test set of size 10000): 0.9685\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = DecisionTree.predict(MNIST_tree_model, X_test)\n",
    "MNIST_rf_accuracy = mean(predicted_labels .== y_test)\n",
    "println(\"\\nPrediction accuracy (measured on test set of size $n_test): \", MNIST_rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at our Fashion MNIST dataset doing exactly the same as above, but tweaking the parameters to achieve the desired accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = fMNIST_train_labels\n",
    "y_test = fMNIST_test_labels\n",
    "\n",
    "n_train, n_test = last(size(y_train)), last(size(y_test))\n",
    "\n",
    "X_train = vcat([vec(fMNIST_train_imgs[:,:,k])' for k in 1:n_train]...)\n",
    "X_test = vcat([vec(fMNIST_test_imgs[:,:,k])' for k in 1:n_test]...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             120\n",
       "n_subfeatures:       90\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             nothing\n",
       "ensemble:            nothing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fMNIST_tree_model = RandomForestClassifier(n_subfeatures = 90, n_trees = 120, partial_sampling=0.9, max_depth=-1, rng=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier\n",
       "n_trees:             120\n",
       "n_subfeatures:       90\n",
       "partial_sampling:    0.9\n",
       "max_depth:           -1\n",
       "min_samples_leaf:    1\n",
       "min_samples_split:   2\n",
       "min_purity_increase: 0.0\n",
       "classes:             [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "ensemble:            Ensemble of Decision Trees\n",
       "Trees:      120\n",
       "Avg Leaves: 3425.225\n",
       "Avg Depth:  25.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTree.fit!(fMNIST_tree_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction accuracy (measured on test set of size 10000): 0.8811\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = DecisionTree.predict(fMNIST_tree_model, X_test)\n",
    "fMNIST_rf_accuracy = mean(predicted_labels .== y_test)\n",
    "println(\"\\nPrediction accuracy (measured on test set of size $n_test): \", fMNIST_rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around with the parameters only seemed to get me to an accuracy of about 0.88, with neglible gains with a significant time penalty so instead of waiting 5+ minutes to see a 0.0001 improvement in accuracy, I will call it here.  Thus, we have a an accuracy for the MNIST dataset is 0.9685 and Fashion MNIST dataset is 0.8811"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
